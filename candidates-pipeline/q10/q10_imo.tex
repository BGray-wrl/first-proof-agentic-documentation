\documentclass[11pt]{article}

% --- Page Layout ---
\usepackage{geometry}
\geometry{margin=1.25in}

% --- Math Packages ---
\usepackage{amsmath, amssymb, amsthm}

% --- Colored Boxes ---
\usepackage[most]{tcolorbox}
\tcbuselibrary{skins, breakable}

% --- Box Colors ---
\definecolor{boxblue}{RGB}{0,0,150}
\definecolor{boxback}{RGB}{245,245,255}

% --- Theorem Environments ---
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

% --- Problem Box ---
\newtcolorbox{problem}[1]{%
  colback=boxback,
  colframe=boxblue,
  fonttitle=\bfseries\large,
  title={#1},
  sharp corners,
  enhanced,
  attach boxed title to top left={yshift=-2mm, xshift=2mm},
  boxed title style={colframe=boxblue, colback=boxblue},
  before skip=15pt plus 2pt,
  after skip=15pt plus 2pt,
  top=10pt, bottom=10pt, left=10pt, right=10pt
}

% --- Result Box (breakable across pages) ---
\newtcolorbox{result}[1]{%
  colback=white,
  colframe=boxblue,
  fonttitle=\bfseries\large,
  title={#1},
  sharp corners,
  enhanced jigsaw,
  breakable,
  attach boxed title to top left={yshift=-2mm, xshift=2mm},
  boxed title style={colframe=boxblue, colback=boxblue},
  before skip=15pt plus 2pt,
  after skip=15pt plus 2pt,
  top=10pt, bottom=10pt, left=10pt, right=10pt
}

\begin{document}

\begin{problem}{User Prompt}
Given a $d$-way tensor $\mathcal{T} \in \mathbb{R}^{n_1 \times n_2 \times \cdots \times n_d}$ 
such that the data is unaligned (meaning the tensor $\mathcal{T}$ has missing entries),
we consider the problem of computing a CP decomposition of rank $r$ where some modes are infinite-dimensional and constrained to be in a Reproducing Kernel Hilbert Space (RKHS). 
We want to solve this using an alternating optimization approach, and our question is focused on the mode-$k$ subproblem for an infinite-dimensional mode. 
For the subproblem, then CP factor matrices 
$A_1, \dots, A_{k-1}, A_{k+1}, \dots, A_d$ are fixed, and we are solving for $A_k$.

Our notation is as follows.
Let $N = \prod_i n_i$ denote the product of all sizes.
Let $n \equiv n_k$ be the size of mode $k$, let
$M = \prod_{i\neq k} n_i$ be the product of all dimensions except $k$, and assume $n \ll M$.
Since the data are unaligned, this means only a subset of $\mathcal{T}$'s entries are observed, and we let $q \ll N$ denote the number of observed entries.
We let $T \in \mathbb{R}^{n \times M}$ denote the mode-$k$ unfolding of the tensor $\mathcal{T}$ with all missing entries set to zero.
The $\operatorname{vec}$ operations creates a vector from a matrix by stacking its columns,
and we let $S \in \mathbb{R}^{N \times q}$ denote the selection matrix (a subset of the $N \times N$ identity matrix) such that $S^T \operatorname{vec}(T)$ selects the $q$ known entries of the tensor $\mathcal{T}$ from the vectorization of its mode-$k$ unfolding.
We let $Z = A_d \odot \cdots \odot A_{k+1} \odot A_{k-1} \odot \cdots \odot A_1 \in \mathbb{R}^{M \times r}$ be the Khatri-Rao product of the factor matrices corresponding to all modes except mode $k$.
We let $B = TZ$ denote the MTTKRP of the tensor $\mathcal{T}$ and Khatri-Rao product $Z$.

We assume $A_k = KW$ where
$K \in \mathbb{R}^{n \times n}$ denotes the psd RKHS kernel matrix for mode $k$.
The matrix $W$ of size $n \times r$ is the unknown for which we must solve. 
The system to be solved is
\begin{equation} 
	\left[
    (Z \otimes K)^T S
    S^T (Z \otimes K)
    + \lambda (I_r \otimes K) 
  \right] \operatorname{vec}(W)
	= (I_r \otimes K) 
	\operatorname{vec}( B ). 
\end{equation}
Here, $I_r$ denotes the $r \times r$ identity matrix.
This is a system of size $nr \times nr$
Using a standard linear solver costs $O(n^3 r^3)$, 
and explicitly forming the matrix is an additional expense.

Explain how an iterative preconditioned conjugate gradient linear solver can be used to solve this problem more efficiently. Explain the method and choice of preconditioner. Explain in detail how the matrix-vector products are computed and why this works. Provide complexity analysis. 
We assume $n,r < q \ll N$. Avoid any computation of order $N$.

\end{problem}

\begin{result}{System Result}

\begin{theorem}[PCG for the RKHS mode-$k$ CP subproblem with missing data]
Let $n:=n_k$, $M:=\prod_{i\neq k} n_i$, and let $\Omega\subset [n]\times[M]$ be the set of observed indices in the mode-$k$ unfolding with $|\Omega|=q$.
Let $S\in\mathbb{R}^{(nM)\times q}$ be the associated selection matrix and define the masking operator
$P_\Omega:\mathbb{R}^{n\times M}\to\mathbb{R}^{n\times M}$ by
\[
(P_\Omega(X))_{ij} :=
\begin{cases}
X_{ij}, & (i,j)\in\Omega,\\
0, & (i,j)\notin\Omega.
\end{cases}
\]
Let $K\in\mathbb{R}^{n\times n}$ be symmetric positive definite and let $\lambda>0$.
Let $Z\in\mathbb{R}^{M\times r}$ be the Khatri--Rao product of the fixed factors in all modes except $k$ and let $B:=TZ\in\mathbb{R}^{n\times r}$.
Consider the linear system for $W\in\mathbb{R}^{n\times r}$:
\begin{equation}\label{eq:system}
\Bigl[(Z\otimes K)^\top SS^\top(Z\otimes K) + \lambda (I_r\otimes K)\Bigr]\operatorname{vec}(W)
= (I_r\otimes K)\operatorname{vec}(B).
\end{equation}
Then:
\begin{enumerate}
\item[(i)] \eqref{eq:system} is equivalent to the matrix equation
\begin{equation}\label{eq:matrix_form}
\mathcal{A}(W) = KB,
\qquad
\mathcal{A}(W) := K\,P_\Omega(KWZ^\top)\,Z + \lambda KW,
\end{equation}
and the induced linear operator $\mathcal{A}:\mathbb{R}^{n\times r}\to\mathbb{R}^{n\times r}$ is self-adjoint and positive definite w.r.t.\ the Frobenius inner product.
\item[(ii)] A preconditioned conjugate-gradient (PCG) method applied to \eqref{eq:system} can be implemented without ever forming matrices of dimension $nM$ or $N=\prod_i n_i$. Each PCG iteration requires
\[
O(n^2 r)\quad\text{work for dense kernel multiplies by $K$, and}
\]
\[
O(q\,r)\quad\text{work to handle missingness},
\]
assuming $d$ is treated as a constant and each observed index provides the corresponding row of $Z$ in $O(r(d-1))$ time.
\item[(iii)] Under uniform random sampling of $\Omega$ with density $\rho := q/(nM)$, the preconditioner
\begin{equation}\label{eq:preconditioner_vec}
\mathcal{M}
:= \rho\,(Z^\top Z\otimes K^2) + \lambda (I_r\otimes K)
\end{equation}
satisfies $\mathbb{E}[(Z\otimes K)^\top SS^\top (Z\otimes K)] = \rho (Z^\top Z\otimes K^2)$ and can be applied in $O(n^2 r + n r^2)$ time after $O(n^3+r^3)$ preprocessing via eigendecompositions.
\end{enumerate}
Consequently, the total cost to reach an $\varepsilon$-accurate solution by PCG is
\[
O\!\left(n^3 + r^3 + \sum_{m\neq k} n_m r^2\right)
\;+\;
O\!\left(k_{\rm iter}\,(n^2 r + q r)\right),
\]
where $k_{\rm iter}$ is the number of PCG iterations, and all computations avoid order-$N$ work.
\end{theorem}

\begin{proof}
\textbf{Step 1: Masking as a projection.}
By construction of the selection matrix, for any $X\in\mathbb{R}^{n\times M}$ one has
\[
SS^\top \operatorname{vec}(X) = \operatorname{vec}(P_\Omega(X)).
\]
Indeed, $SS^\top$ is the diagonal projector on the coordinates indexed by $\Omega$.

\textbf{Step 2: Rewriting the operator in matrix form.}
Let $W\in\mathbb{R}^{n\times r}$. Using the characteristic Kronecker identity
\begin{equation}\label{eq:kronecker_vec}
(A\otimes B)\operatorname{vec}(X)=\operatorname{vec}(BXA^\top),
\end{equation}
(valid for conforming dimensions; see \cite[Eq.~(S1)]{wenger_hennig_2020_supp}),
we obtain
\[
(Z\otimes K)\operatorname{vec}(W)
= \operatorname{vec}(KWZ^\top)\in\mathbb{R}^{nM}.
\]
Applying the mask and then the adjoint gives
\[
(Z\otimes K)^\top SS^\top (Z\otimes K)\operatorname{vec}(W)
= (Z^\top\otimes K)\operatorname{vec}(P_\Omega(KWZ^\top))
= \operatorname{vec}\!\bigl(K\,P_\Omega(KWZ^\top)\,Z\bigr),
\]
where \eqref{eq:kronecker_vec} is used again with $A=Z^\top$, $B=K$.
Also,
\[
(I_r\otimes K)\operatorname{vec}(W) = \operatorname{vec}(KW),
\qquad
(I_r\otimes K)\operatorname{vec}(B) = \operatorname{vec}(KB)
\]
by \eqref{eq:kronecker_vec} with $A=I_r$, $B=K$.
Therefore \eqref{eq:system} is equivalent to \eqref{eq:matrix_form}.

\textbf{Step 3: Symmetry and positive definiteness.}
Since $K$ is symmetric and $P_\Omega$ is an orthogonal projector in the Frobenius inner product,
$\mathcal{A}$ is self-adjoint.
For any $W\neq 0$,
\[
\langle W,\mathcal{A}(W)\rangle_F
=
\langle W, K P_\Omega(KWZ^\top) Z\rangle_F + \lambda \langle W, KW\rangle_F.
\]
Using \eqref{eq:kronecker_vec} and Step 1, the first term equals
\[
\langle (Z\otimes K)\operatorname{vec}(W),\, SS^\top (Z\otimes K)\operatorname{vec}(W)\rangle
= \| SS^\top (Z\otimes K)\operatorname{vec}(W)\|_2^2 \ge 0.
\]
The second term satisfies $\langle W, KW\rangle_F = \operatorname{tr}(W^\top K W) > 0$ because $K\succ 0$ and $W\neq 0$.
Hence $\langle W,\mathcal{A}(W)\rangle_F>0$, so $\mathcal{A}$ (and the matrix in \eqref{eq:system}) is positive definite.
Therefore CG/PCG is applicable.

\textbf{Step 4: Matrix-vector products in $O(n^2r + qr)$ time.}
PCG requires repeated evaluation of $v\mapsto \mathcal{A}v$, equivalently $V\mapsto \mathcal{A}(V)$ for $V\in\mathbb{R}^{n\times r}$.
Let $Y:=KV$ (cost $O(n^2r)$ if $K$ is dense).
We must compute $R := P_\Omega(YZ^\top)\,Z\in\mathbb{R}^{n\times r}$ without forming $YZ^\top$ (an $n\times M$ matrix).
Write $\Omega=\{(i_t,j_t)\}_{t=1}^q$.
Then, for each observed pair $(i_t,j_t)$,
\[
(P_\Omega(YZ^\top))_{i_t j_t} = (YZ^\top)_{i_t j_t} = \langle Y_{i_t,:}, Z_{j_t,:}\rangle,
\]
and
\[
R_{i_t,:}\;\; \mathrel{+}= \;\; (YZ^\top)_{i_t j_t}\; Z_{j_t,:}.
\]
Thus one pass over $\Omega$ accumulates $R$ in $O(qr)$ arithmetic given $Z_{j_t,:}$.

Crucially, $Z$ need not be formed explicitly.
Each $j_t$ corresponds to a $(d-1)$-tuple of indices $(i_1,\dots,i_{k-1},i_{k+1},\dots,i_d)$, and the corresponding row equals the elementwise product
\[
Z_{j_t,:} = A_d(i_d,:)\;*\;\cdots\;*\;A_{k+1}(i_{k+1},:)\;*\;A_{k-1}(i_{k-1},:)\;*\;\cdots\;*\;A_1(i_1,:),
\]
so retrieving $Z_{j_t,:}$ costs $O(r(d-1))$ and does not depend on $M$ or $N$; cf.\ the definition of the Khatri--Rao product and its basic properties \cite[\S2.6]{kolda_bader_2009}.
Finally, compute $K R$ (cost $O(n^2r)$) and add $\lambda Y$:
\[
\mathcal{A}(V) = K R + \lambda Y.
\]
Overall, one operator application costs $O(n^2r + qr)$ (treating $d$ as constant).

\textbf{Step 5: Computing $B=TZ$ without order-$N$ work (optional completeness).}
Let the observed entries of the mode-$k$ unfolding be $\{(i_t,j_t, T_{i_t j_t})\}_{t=1}^q$.
Then
\[
B = TZ = \sum_{t=1}^q T_{i_t j_t}\, e_{i_t}\, Z_{j_t,:},
\]
so $B$ can be accumulated in one pass over observations in $O(qr)$ time, again without forming $T$ or $Z$.

\textbf{Step 6: Preconditioner and its efficient application.}
Assume the observed set $\Omega$ is drawn uniformly at random with replacement from $[n]\times [M]$ and write $\rho := q/(nM)$.
Then
\[
SS^\top = \sum_{t=1}^q e_{\ell_t} e_{\ell_t}^\top,\qquad \ell_t\in[nM],
\]
and therefore
\[
(Z\otimes K)^\top SS^\top (Z\otimes K)
=
\sum_{t=1}^q a_{\ell_t} a_{\ell_t}^\top,\qquad a_{\ell}:=(Z\otimes K)^\top e_\ell.
\]
Taking expectations and using independence,
\[
\mathbb{E}\bigl[(Z\otimes K)^\top SS^\top (Z\otimes K)\bigr]
= q\,\mathbb{E}_{\ell\sim\mathrm{Unif}[nM]}[a_\ell a_\ell^\top].
\]
A direct computation with $\ell\equiv(i,j)$ and \eqref{eq:kronecker_vec} yields
$\mathbb{E}[a_\ell a_\ell^\top]=\frac{1}{nM}(Z^\top Z\otimes K^2)$, hence
\[
\mathbb{E}\bigl[(Z\otimes K)^\top SS^\top (Z\otimes K)\bigr] = \rho (Z^\top Z\otimes K^2),
\]
which motivates \eqref{eq:preconditioner_vec}.
(If desired, concentration of this random sum around its mean can be obtained from a matrix Bernstein inequality; see \cite[Theorem~1.4]{tropp_2011_tail}.)

To apply $\mathcal{M}^{-1}$ efficiently, observe first that
\begin{equation}\label{eq:ZtZ_gram}
Z^\top Z
=
\underset{m\neq k}{\ast}\,(A_m^\top A_m),
\end{equation}
where $\ast$ denotes the Hadamard product; this is the Khatri--Rao Gram identity \cite[\S2.6, identity $(A\odot B)^\top(A\odot B)=A^\top A\ast B^\top B$]{kolda_bader_2009}.
Thus $Z^\top Z$ can be formed in $O(\sum_{m\neq k} n_m r^2)$ time without constructing $Z$.

Next, take eigendecompositions
\[
K = U \Sigma U^\top,\qquad Z^\top Z = Q \Gamma Q^\top,
\]
with $\Sigma=\mathrm{diag}(\sigma_1,\dots,\sigma_n)$ and $\Gamma=\mathrm{diag}(\gamma_1,\dots,\gamma_r)$.
Using standard Kronecker diagonalization rules (also recorded in \cite[Proposition~S1, Eq.~(S10)]{wenger_hennig_2020_supp}),
\[
\mathcal{M} = (Q\otimes U)\,\mathrm{diag}\!\bigl(\rho\,\gamma_j \sigma_i^2 + \lambda \sigma_i\bigr)_{i\in[n],\,j\in[r]}\,(Q^\top\otimes U^\top).
\]
Hence, given a residual vector reshaped as $R\in\mathbb{R}^{n\times r}$, the solution $X=\mathcal{M}^{-1}R$ is obtained by
\[
\widetilde{R} = U^\top R Q,\qquad
\widetilde{X}_{ij} = \frac{\widetilde{R}_{ij}}{\rho\,\gamma_j\sigma_i^2 + \lambda \sigma_i},
\qquad
X = U \widetilde{X} Q^\top.
\]
This costs $O(n^2 r + n r^2)$ per application after $O(n^3+r^3)$ preprocessing.

\textbf{Step 7: PCG convergence and total complexity.}
Since $\mathcal{A}$ is SPD, PCG produces iterates $W^{(t)}$ in the Krylov subspaces and satisfies standard error bounds in the energy norm in terms of the (preconditioned) condition number; see, e.g., \cite[Theorem~2.2]{nishimura_2018_cg}.
Combining Step 4 and Step 6 yields per-iteration cost $O(qr+n^2r)$ (dominant terms), and the stated total complexity follows.
All operations depend on $(n,r,q)$ and the mode sizes $\{n_m\}$, and avoid order-$N$ work.
\end{proof}

\begin{thebibliography}{10}

\bibitem{kolda_bader_2009}
T.~G.~Kolda and B.~W.~Bader.
\newblock Tensor decompositions and applications.
\newblock \emph{SIAM Review}, 51(3):455--500, 2009.

\bibitem{wenger_hennig_2020_supp}
J.~Wenger and P.~Hennig.
\newblock Supplementary material for \emph{Probabilistic linear solvers for machine learning}.
\newblock NeurIPS 2020.
\newblock In particular, Eq.~(S1) and Proposition~S1.

\bibitem{nishimura_2018_cg}
A.~Nishimura, D.~Z.~M.\ Sussman, and M.~A.\ Stephens.
\newblock Conjugate gradient convergence for Bayesian linear regression.
\newblock \emph{arXiv:1810.12437}, 2018.
\newblock In particular, Theorem~2.2.

\bibitem{tropp_2011_tail}
J.~A.~Tropp.
\newblock User-friendly tail bounds for sums of random matrices.
\newblock \emph{arXiv:1004.4389}, 2011.
\newblock In particular, Theorem~1.4 (Matrix Bernstein).

\bibitem{paige_saunders_1975}
C.~C.~Paige and M.~A.~Saunders.
\newblock Solution of sparse indefinite systems of linear equations.
\newblock \emph{SIAM Journal on Numerical Analysis}, 12(4):617--629, 1975.

\end{thebibliography}

\end{result}




\end{document}