Spectral Dominance and Vertex-Induced Subgraph Laplacians: An Analysis of the $\epsilon$-Light Subset Conjecture within the "First Proof" InitiativeThe evolution of artificial intelligence has historically been measured against benchmarks that often rely on contest-level mathematics, where problems are designed to have elegant, discoverable solutions within specific time constraints. However, the true frontier of mathematical capability lies in the domain of research-level mathematics—problems that emerge organically from ongoing investigations and whose solutions are not yet part of the public record. In February 2025, a group of distinguished mathematicians introduced the "First Proof" initiative to address this gap, presenting ten research-level questions to evaluate whether modern AI systems can independently navigate the complexities of authentic mathematical discovery. Among these challenges is a profound inquiry into spectral graph theory concerning the existence of $\epsilon$-light subsets in graph Laplacians. This problem, rooted in the foundational work of Daniel Spielman and Nikhil Srivastava, explores the structural limitations of vertex-induced subgraphs and their ability to spectrally approximate their parent graphs.The First Proof Initiative: A New Paradigm for AI EvaluationThe "First Proof" project represents a coordinated effort by twelve mathematicians from institutions such as Stanford, Harvard, Yale, and Columbia to operationalize the assessment of AI in the context of genuine research. The methodology departs from existing frameworks like FrontierMath or IMProofBench, which often focus on automatically gradable tasks or problems with known solutions. The core premise of First Proof is the use of unpublished, research-derived questions whose answers are encrypted and scheduled for release only after a period of community experimentation.Institutional and Collaborative ContextThe authors of the paper, titled "First Proof," are prominent figures in fields ranging from algebraic topology to numerical linear algebra. Their collective expertise ensures that the questions provided reflect the actual distribution of problems faced by working mathematicians. The collaboration includes experts who have previously solved long-standing problems, such as the Kadison-Singer conjecture, providing a high level of rigor to the benchmark.AuthorPrimary InstitutionResearch DomainMohammed AbouzaidStanford UniversitySymplectic GeometryAndrew J. BlumbergColumbia UniversityAlgebraic TopologyMartin HairerEPFL / Imperial CollegeStochastic AnalysisJoe KileelUniversity of Texas at AustinTensor AnalysisTamara G. KoldaMathSci.aiNumerical Linear AlgebraPaul D. NelsonAarhus UniversityNumber TheoryDaniel SpielmanYale UniversitySpectral Graph TheoryNikhil SrivastavaUC BerkeleyCombinatorics / Spectral TheoryRachel WardUniversity of Texas at AustinMachine Learning / SubsamplingShmuel WeinbergerUniversity of ChicagoGeometric TopologyLauren WilliamsHarvard UniversityAlgebraic CombinatoricsThe initiative emphasizes that mathematical research is not merely about finding a numerical answer but about developing novel theories, formalizing definitions, and constructing rigorous proofs. By keeping the solutions secret until February 13, 2026, the authors have created a "contamination-free" environment where AI performance must be the result of genuine reasoning rather than data retrieval.Methodology of the First Proof ExperimentThe experiment invites the broader community to attempt the ten questions using state-of-the-art AI models, such as GPT-5.2 Pro and Gemini 3.0 Deepthink. Participants are encouraged to share full interaction transcripts to illuminate the prompting strategies and methodologies that lead to successful proofs. Unlike contest-style benchmarks, these problems may require multiple pages of derivation and the synthesis of results from disparate areas of mathematics. The "First Proof" title itself is a culinary metaphor for the initial fermentation of dough, suggesting that these questions represent the early stages of a more comprehensive framework for mathematical AI evaluation.Foundations of Spectral Graph Theory: The Laplacian OperatorTo understand the specific question regarding $\epsilon$-light subsets, one must first establish the role of the Laplacian matrix in graph theory. For an undirected graph $G = (V, E)$, the Laplacian matrix $L$ is defined as $L = D - A$, where $D$ is the diagonal degree matrix and $A$ is the adjacency matrix. The spectral properties of $L$ encapsulate the graph's connectivity, diffusion characteristics, and structural bottlenecks.The quadratic form associated with the Laplacian is a fundamental tool for measuring the variation of a signal $x \in \mathbb{R}^V$ across the graph's edges. This form, known as the Dirichlet energy, is expressed as:$$x^\top L x = \sum_{(u,v) \in E} (x_u - x_v)^2$$
This formula demonstrates that the Laplacian is positive semidefinite (PSD), as the energy is always a sum of squares and thus non-negative. The eigenvalues of $L$ provide insights into the graph's geometry; for instance, the second smallest eigenvalue ($\lambda_2$) is a proxy for the graph's algebraic connectivity, while the range of the spectrum is related to the graph's maximum degree.Convergence and Geometric ContextIn the limit of large-scale random geometric graphs, the graph Laplacian converges to the Laplace-Beltrami operator on the underlying manifold from which the vertices were sampled. This convergence connects discrete graph analysis with continuous differential geometry, allowing researchers to apply tools from heat kernel analysis and spectral geometry to discrete datasets. The spectrum of the Laplace-Beltrami operator is a global invariant that encodes key topological properties of the manifold, remaining unchanged under isometric deformations.The study of graph Laplacians is central to numerous applications, including:Semi-supervised Learning: Utilizing the smoothness of labels across a graph to infer missing values.Dimensionality Reduction: Mapping high-dimensional data into lower-dimensional spaces while preserving local geometry (e.g., Laplacian Eigenmaps).Clustering: Identifying communities or modules within a network based on spectral gaps.Network Robustness: Evaluating how failures in vertices or edges impact the overall connectivity and stability of the system.The $\epsilon$-Light Subset Problem: Definitions and FormalismsThe specific query posed within the "First Proof" framework concerns a subset of vertices $S \subseteq V$ and its induced subgraph $G_S = (V, E(S,S))$. Here, $E(S,S)$ represents the set of edges in $G$ where both endpoints reside in $S$. The Laplacian of this induced subgraph is denoted by $L_S$. Because $G_S$ is a subgraph of $G$, its edge set is a subset of $E$.The core definition of the problem is the concept of an $\epsilon$-light subset. A set of vertices $S$ is said to be $\epsilon$-light if the matrix $\epsilon L - L_S$ is positive semidefinite. In terms of quadratic forms, this condition requires that for every possible vector $x \in \mathbb{R}^V$:
$$x^\top L_S x \leq \epsilon x^\top L x$$Substituting the definition of the Dirichlet energy, the condition becomes:$$\sum_{(u,v) \in E(S,S)} (x_u - x_v)^2 \leq \epsilon \sum_{(u,v) \in E} (x_u - x_v)^2$$
This inequality must hold for all $x$, not just for specific indicators or cuts. It implies that the induced subgraph $G_S$ is spectrally "thin" or "dominated" by the parent graph $G$ by a factor of $\epsilon$. The central question is whether there exists a constant $c > 0$ such that for any graph $G$ and any $\epsilon \in (0, 1)$, there always exists an $\epsilon$-light subset $S$ with a size of at least $c \epsilon |V|$.Significance of the Constant cThe existence of a universal constant $c$ would imply a fundamental structural property of graphs: that they always contain large regions that are spectrally representative yet significantly less "active" than the whole. This has implications for sampling, network throttling, and load shedding in large-scale systems. If $c$ exists, it suggests that spectral dominance can be achieved by a vertex subset whose size scales linearly with $\epsilon$.To contextualize this, consider the case where $\epsilon$ is very small. The condition $L_S \preceq \epsilon L$ would require $S$ to be a set of vertices with very few internal edges. In the extreme case where $\epsilon \to 0$, $S$ would need to be an independent set (a set with no internal edges), making $L_S$ the zero matrix. The size of the maximum independent set is $\alpha(G)$. If the conjecture holds, we would need $\alpha(G) \geq c \epsilon |V|$ for a sufficiently small $\epsilon$. However, the conjecture must hold for all $\epsilon \in (0, 1)$, suggesting a much deeper interplay between vertex selection and spectral norm control.Historical Context: From Cut Sparsification to Spectral DominanceThe inquiry into $\epsilon$-light subsets follows a long lineage of research into graph sparsification. Graph sparsification aims to approximate a dense graph with a sparse one while preserving certain essential properties, such as cut values or spectral energy.Cut Sparsifiers and the Benczúr-Karger FrameworkThe earliest forms of sparsifiers were designed to preserve the values of all cuts in a graph. Given a graph $G$ and an error parameter $\epsilon$, a cut sparsifier is a weighted subgraph $G_\epsilon$ such that the weight of every cut in $G_\epsilon$ is within a $(1 \pm \epsilon)$ factor of the corresponding cut in $G$. Benczúr and Karger demonstrated that every graph possesses a cut sparsifier with $O(n \log n / \epsilon^2)$ edges. This work relied on the concept of edge connectivity and "strong components" to determine the sampling probability of each edge.Spectral Sparsification and the Spielman-Teng BreakthroughSpectral sparsification is a more rigorous requirement than cut sparsification. While cut sparsifiers only need to approximate the Laplacian quadratic forms for indicator vectors of sets (which represent cuts), spectral sparsifiers must approximate the forms for all vectors $x \in \mathbb{R}^V$. This is equivalent to saying that the Laplacians of the two graphs are spectrally close:$$(1-\epsilon)L_G \preceq L_{G_\epsilon} \preceq (1+\epsilon)L_G$$
Spielman and Teng provided the first construction of spectral sparsifiers with a nearly linear number of edges. Their approach utilized preconditioning and the solution of Laplacian systems of equations. This was later improved by Batson, Spielman, and Srivastava, who showed that every graph has a spectral sparsifier with $O(n/\epsilon^2)$ edges—a linear number of edges relative to the vertex set $n$.Sparsification TypeProperty PreservedEdge CountKey MethodologyCut SparsifierCut values $\delta(U)$Spectral SparsifierQuadratic forms $x^\top L x$$O(n / \epsilon^2)$Effective Resistance / Barrier Functions$\epsilon$-Light SubsetDominance $L_S \preceq \epsilon L$$SThe problem of $\epsilon$-light subsets differs from standard spectral sparsification in its constraint. In sparsification, one typically selects a set of edges and assigns them weights. In the $\epsilon$-light subset problem, one must select a set of vertices and include all internal edges unweighted. This makes the problem inherently combinatorial and more restrictive.The Kadison-Singer Breakthrough and the Method of Interlacing PolynomialsThe transition from edge-weighted sparsifiers to more constrained structural properties was catalyzed by the resolution of the Kadison-Singer problem. For decades, the Kadison-Singer conjecture remained an open question in $C^*$-algebras and quantum mechanics. In 2013, Marcus, Spielman, and Srivastava proved that the conjecture was true by formulating it as a question about the existence of certain partitions of vectors in linear algebra.The core of their proof was the method of interlacing families of polynomials. They showed that for any set of vectors $v_1, \dots, v_m$ that form a partition of the identity, there exists a way to partition them into two sets $S_1, S_2$ such that the maximum eigenvalue of the sum of outer products in each set is bounded by a constant factor. This technique allowed for the existence of sparsifiers to be proven without relying on randomized construction, but rather on the properties of characteristic polynomials.Application to Spectral Graph TheoryThe Kadison-Singer result directly implies that one can always find a "thin" partition of edges. The $\epsilon$-light subset question asks for a "thin" subset of vertices. Because the Laplacian $L$ can be decomposed as a sum of outer products of edge vectors (i.e., $L = \sum_{(u,v) \in E} (e_u - e_v)(e_u - e_v)^\top$), the selection of a subset of edges is equivalent to selecting a subset of these rank-1 matrices. However, selecting a vertex subset $S$ implies selecting all edges $(u,v)$ where $u,v \in S$. This introduces a quadratic dependency that is not present in the original Kadison-Singer framework.The inclusion of Nikhil Srivastava and Daniel Spielman as authors of the "First Proof" paper strongly suggests that the $\epsilon$-light subset problem is intended to test whether AI can extend the logic of interlacing polynomials to these more complex, vertex-induced constraints.Vertex-Induced Subgraphs vs. Edge-Based SparsifiersA critical distinction in graph theory is the difference between a subgraph and a vertex-induced subgraph. A general subgraph can be formed by taking any subset of edges. A vertex-induced subgraph $G_S$ is formed by taking a subset of vertices $S$ and all edges of the original graph that have both endpoints in $S$.The Combinatorial Constraint of Induced SubgraphsIn the context of spectral approximation, selecting edges allows for precise control over the resulting Laplacian. One can select edges that have low "effective resistance"—a measure of an edge's importance to the graph's spectral structure—to ensure the sparsifier remains accurate. In vertex-induced subgraphs, selecting a vertex "forces" the inclusion of all its neighbors that are also in $S$.If a graph is very dense (e.g., a complete graph $K_n$), any large vertex subset will induce many edges. For $K_n$, the Laplacian $L$ has eigenvalues $n$ and $0$. An induced subgraph on $k$ vertices is $K_k$, whose Laplacian $L_S$ (within the $n \times n$ context) has eigenvalues $k$ and $0$. The condition $L_S \preceq \epsilon L$ would roughly translate to $k \leq \epsilon n$. This aligns with the conjecture's requirement of $|S| \geq c \epsilon |V|$, where $c \approx 1$ for complete graphs.However, for more sparse or irregular graphs, the constraint is more complex. In cubic graphs (where every vertex has degree 3), researchers have investigated the size of maximum induced forests. An induced forest is a vertex-induced subgraph with no cycles. Because forests have very low spectral energy, they are natural candidates for $\epsilon$-light subsets for small $\epsilon$.Graph PropertyCubic Graph ResultImplications for ϵ-LightnessVertex Count$\rho$ verticesBase size for subset calculationInduced Forest Size$\geq (5\rho - 2)/8$Large induced set with low energy Triangle-Free Bound$\geq (2\rho - 1)/3$Improvement for sparse, cycle-limited graphsSpectral CharacteristicNo cycles means low eigenvaluesLikely satisfies $L_S \preceq \epsilon L$ for small $\epsilon$The existence of large induced forests in bounded-degree graphs provides a baseline for the conjecture. If one can always find an induced subgraph that is essentially a forest and contains a constant fraction of the vertices, then for some $\epsilon$, the condition would be satisfied. The challenge of the conjecture is to show this holds for all $\epsilon$ and all graphs, including expanders where every large subset induces many edges.Spectral Thinness and the Held-Karp RelaxationThe study of $\epsilon$-light subsets is also linked to the concept of "spectral thinness," which has been explored in the context of the Asymmetric Traveling Salesman Problem (ATSP). In the ATSP, one seeks the shortest tour that visits all vertices in a directed graph. The integrality gap of the standard LP relaxation (the Held-Karp relaxation) has long been conjectured to be a constant.Combinatorial vs. Spectral Thin TreesA spanning tree $T$ is said to be $\alpha$-combinatorially thin if for every cut $S \subset V$, the number of edges of $T$ in the cut is at most an $\alpha$ fraction of the total edges in that cut in the original graph $G$. A more powerful concept is spectral thinness: a tree $T$ is $\alpha$-spectrally thin if its Laplacian $L_T$ satisfies $L_T \preceq \alpha L_G$.The integrality gap of the Held-Karp relaxation is bounded by the thinness of trees in the graph. In 2010, Asadpour et al. improved the approximation factor for ATSP using the connection to thin trees. Later, it was shown that any $k$-edge-connected graph has a $(\text{polyloglog}(n)/k)$-thin spanning tree. The jump from combinatorial thinness to spectral thinness is significant because spectral thinness implies combinatorial thinness for all cuts, but the converse does not necessarily hold.The "First Proof" question regarding $\epsilon$-light subsets can be viewed as an inquiry into "induced thinness." Instead of looking for a spanning tree (which uses $n-1$ edges), it looks for a vertex-induced subgraph (which could have many more edges) that satisfies a similar spectral dominance condition. This suggests that the solution may involve the same "barrier function" techniques used to prove the existence of spectrally thin trees.Analysis of Subsampling and Concentration of MeasureOne potential approach to finding an $\epsilon$-light subset is through randomized subsampling. This technique is widely used in numerical linear algebra and machine learning to reduce the complexity of large datasets. Rachel Ward, one of the authors of the "First Proof" paper, has contributed extensively to the theory of subsampling and its impact on the convergence of stochastic algorithms.The Mechanism of Random SelectionIf we select each vertex $v \in V$ to be in $S$ with probability $p$, the expected size of $S$ is $p |V|$. For any edge $(u,v) \in E$, the probability that it is included in the induced subgraph $G_S$ is $p^2$, as both endpoints must be chosen independently. Therefore, the expected Laplacian of the induced subgraph is:$$E = p^2 L$$To satisfy the condition $E \preceq \epsilon L$ on average, we would set $p^2 = \epsilon$, or $p = \sqrt{\epsilon}$. This would result in a subset $S$ of expected size $\sqrt{\epsilon} |V|$.However, the conjecture asks for a subset of size $c \epsilon |V|$, which is smaller than $\sqrt{\epsilon} |V|$ for small $\epsilon$. This indicates that a simple random selection might be "too heavy"—it includes too many edges for the spectral condition to hold with high probability. A more biased or sophisticated sampling method might be required to ensure that the realized matrix $L_S$ does not exceed $\epsilon L$ in the PSD sense.Matrix Concentration InequalitiesProving that a random matrix stays close to its expectation involves matrix concentration inequalities, such as the Matrix Chernoff Bound or the Matrix Bernstein Inequality. These tools provide bounds on the probability that the maximum eigenvalue of a sum of random matrices deviates from its mean. In the case of $L_S$, the terms in the sum are not independent, as the inclusion of an edge $(u,v)$ is tied to the selection of vertices $u$ and $v$. This dependency makes the application of standard concentration bounds challenging and points toward the need for the "interlacing" techniques mentioned previously.| Subsampling Strategy | Expected $L_S$ | Expected $|S|$ | Challenge || :--- | :--- | :--- | :--- || Uniform Random ($p$) | $p^2 L$ | $p |V|$ | Variance in eigenvalues may violate PSDness || Importance Sampling | $\sum w_i L_i$ | $\sum p_i$ | Defining weights for vertices to preserve cuts || Barrier-Guided | Deterministic | Target Size | High computational cost to find $S$ |The research into "well-spread" codes and the delocalization of eigenvectors also provides clues. If the eigenvectors of $L$ are spread thinly across many vertices, it is less likely that any specific subset $S$ will cause a massive spike in the quadratic form $x^\top L_S x$, making the $\epsilon$-light condition easier to satisfy.The Role of Connectivity and ExpandersThe structure of the original graph $G$ significantly influences the existence and size of $\epsilon$-light subsets. Expanders—graphs that are highly connected and have no small bottlenecks—represent the most difficult case for this conjecture. In an expander, any large subset of vertices will have many internal edges, potentially making $L_S$ "heavy."Expander Decompositions and ClusterabilityA recent line of work has focused on expander decompositions: partitioning a graph into several induced subgraphs, each of which is an expander, by removing only a small fraction of edges. A graph is considered $(k, \epsilon)$-clusterable if its vertex set can be partitioned into $k$ induced expanders with low outer conductance.In the context of the $\epsilon$-light subset problem, if we can partition $G$ into several components that are spectrally similar, we might be able to pick a subset $S$ from one of these components. However, the requirement is that $L_S$ is dominated by the global Laplacian $L$. This means $S$ must be light relative to the total connectivity of the graph, not just its local component.Disconnecting Sets and Module IdentifiabilityIn networking, the concept of a "disconnecting set" is used to identify modules and allocate excitation signals for monitoring. If $S$ is chosen such that it does not bridge major disconnecting sets, its internal connectivity will be limited, helping it satisfy the $\epsilon$-lightness condition. This is particularly relevant in power transmission and distribution networks, where real-time monitoring of voltage and currents depends on the spectral properties of the network model.Computational Aspects: SDDM Solvers and PreconditioningBeyond the theoretical existence of $c$, the $\epsilon$-light subset problem has practical implications for the design of efficient algorithms. Symmetric Diagonally Dominant (SDD) matrices, which include Laplacians, are central to many scientific computing tasks. Solving systems $Lx = b$ can be done in nearly-linear time using algorithms that leverage spectral sparsification.Laplacian System SolversThe fastest known solvers for Laplacian systems achieve running times of $O(m \log^k n)$ where $m$ is the number of edges. These solvers often use a preconditioning approach: they find a simpler graph (a sparsifier or a tree) that approximates $L$ and use its inverse to accelerate the convergence of iterative methods like Conjugate Gradient.Solver DevelopmentRunning TimeInnovationSpielman-Teng (2004)Nearly-LinearFirst sparsification-based solverKoutis-Miller-Tolliver$O(m \log n)$Combinatorial multigridSDDM2023 (Spielman)Nearly-LinearPublic benchmark and simplified Cholesky Nonlinear Extension$O(k^2 m n \log(n/\epsilon))$Iterative duality for nonlinear Laplacians If the $\epsilon$-light subset conjecture is true, it suggests a new way to precondition Laplacian systems. By identifying a large, spectrally light subset $S$, one could potentially simplify the matrix $L$ by focusing on the interactions within $S$ or by using $G_S$ as a base for a hierarchical solver. The search for these subsets is closely related to the "Max-Cut" problem and other combinatorial optimization tasks that attempt to partition vertices to maximize or minimize specific edge counts.The Implications of the "First Proof" Project for the Future of ResearchThe $\epsilon$-light subset problem is more than just a question in spectral graph theory; it is a test case for the future of human-AI collaboration in mathematics. The authors of the "First Proof" paper argue that current mathematical benchmarks suffer from "data contamination," where AI models may recall solutions from their training data rather than genuinely solving problems. By using unpublished research problems, they aim to create a true measure of reasoning capability.Implications for AI and Research WorkflowsThe initiative has potential applications across multiple sectors:Academic and Educational: Graduate courses in mathematics and computer science can use these questions to teach advanced prompting, proof verification, and responsible AI use.Scientific R&D: Research teams can use the First Proof protocol as a vendor-neutral test suite to procure AI assistants capable of handling "unknown-problem" evaluations.Autonomous Discovery: If AI systems can solve these problems, it suggests they can eventually assist in the most creative stages of research, such as selecting which questions to study and developing novel theories.The "First Proof" project acknowledges that ten questions are not sufficient for a comprehensive statistical assessment, but it establishes a methodology that can be scaled as the field of mathematical AI matures. The transition from "contest mathematics" to "research mathematics" is crucial because research problems reflect the formulation, ambiguity, and depth intrinsic to the actual mathematical process.Analysis and Future OutlookThe question of whether every graph $G$ contains an $\epsilon$-light subset of size $c \epsilon |V|$ remains an active research challenge. The available evidence from spectral sparsification and the Kadison-Singer solution points toward a positive answer, likely involving a constant $c$ that depends on the graph's maximum degree or its spectral gap. However, the requirement of a vertex-induced subgraph introduces a layer of combinatorial complexity that edge-weighted sparsifiers do not face.The resolution of this problem will likely require a synthesis of:Barrier Function Methods: Defining a potential function that prevents the eigenvalues of $L_S$ from growing too large as vertices are added to $S$.Advanced Matrix Concentration: Developing new bounds for sums of matrices with the specific dependencies found in induced subgraphs.Spectral Geometry: Utilizing the convergence of the graph Laplacian to the Laplace-Beltrami operator to find geometric counter-examples or proof paths.As the February 13, 2026 release date for the encrypted solutions approaches, the community continues to probe the limits of these spectral operators. Whether the answer is a simple "yes" or a complex conditional statement, the inquiry itself has already advanced our understanding of how discrete vertex sets can capture the global spectral energy of a network. The $\epsilon$-light subset conjecture stands as a testament to the enduring power of spectral graph theory to reveal the hidden order within complex structures.

Works cited
First Proof | alphaXiv, accessed February 12, 2026, https://www.alphaxiv.org/overview/2602.05192
First Proof: AI Evaluation in Research Math - Emergent Mind, accessed February 12, 2026, https://www.emergentmind.com/papers/2602.05192
[2602.05192] First Proof - arXiv, accessed February 12, 2026, https://arxiv.org/abs/2602.05192
First Proof - arXiv, accessed February 12, 2026, https://arxiv.org/html/2602.05192v1
First Proof - Hacker News, accessed February 12, 2026, https://news.ycombinator.com/item?id=46924591
Mathematicians Create New AI Math Test With Unpublished Problems - Can AI Really Solve Research Problems? - UNU Campus Computing Centre, accessed February 12, 2026, https://c3.unu.edu/blog/mathematicians-create-new-ai-math-test-with-unpublished-problems
Graph Laplacians and their convergence on random neighborhood graphs - ResearchGate, accessed February 12, 2026, https://www.researchgate.net/publication/2130520_Graph_Laplacians_and_their_convergence_on_random_neighborhood_graphs
Convergence of the density of states and delocalization of eigenvectors on random regular graphs - ResearchGate, accessed February 12, 2026, https://www.researchgate.net/publication/236627640_Convergence_of_the_density_of_states_and_delocalization_of_eigenvectors_on_random_regular_graphs
Hyperbolic Space Spectral Characteristics in a Network of Mechanical Linkages - arXiv, accessed February 12, 2026, https://arxiv.org/html/2402.04531v1
Theory of Computation Laboratory 4 - Infoscience - EPFL, accessed February 12, 2026, https://infoscience.epfl.ch/entities/orgunit/9faf92da-88ea-4282-94df-5c26cc089f03
A Robust Spectral Clustering Algorithm for Sub-Gaussian Mixture Models with Outliers, accessed February 12, 2026, https://www.researchgate.net/publication/361911362_A_Robust_Spectral_Clustering_Algorithm_for_Sub-Gaussian_Mixture_Models_with_Outliers
Almost-Linear-Time Algorithms for Maximum Flow and Minimum-Cost Flow - ResearchGate, accessed February 12, 2026, https://www.researchgate.net/publication/375731152_Almost-Linear-Time_Algorithms_for_Maximum_Flow_and_Minimum-Cost_Flow
A GENERAL FRAMEWORK FOR GRAPH SPARSIFICATION\ast - UBC Computer Science, accessed February 12, 2026, https://www.cs.ubc.ca/~nickhar/papers/Sparsifier/Sparsifier-Long.pdf
Combinatorially Thin Trees and Spectrally Thin Trees in Structured Graphs - UWSpace - University of Waterloo, accessed February 12, 2026, https://uwspace.uwaterloo.ca/bitstreams/206ed233-6b52-4116-9297-63c14ca7fbf5/download
Robust and Practical Solution of Laplacian Equations by Approximate Elimination - MPG.PuRe, accessed February 12, 2026, https://pure.mpg.de/rest/items/item_3495915_2/component/file_3495916/content
New Approaches to the Asymmetric Traveling Salesman and Related Problems - UC Berkeley EECS, accessed February 12, 2026, https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-2.pdf
Lower Bounds For Induced Forests in Cubic Graphs - ResearchGate, accessed February 12, 2026, https://www.researchgate.net/publication/265162613_Lower_Bounds_For_Induced_Forests_in_Cubic_Graphs
Lecture 13: A Construction of Linear size Thin Forests 13.1 Combinatorial and Spectral Thin Trees, accessed February 12, 2026, https://homes.cs.washington.edu/~shayan/courses/approx/adv-approx-13.pdf
Effective-Resistance-Reducing Flows, Spectrally Thin Trees, and Asymmetric TSP - arXiv, accessed February 12, 2026, https://arxiv.org/pdf/1411.4613
Keyword Clouds by Papers - ICML Beijing, accessed February 12, 2026, https://icml.cc/2014/icml2014keywords/index/article/all.html
One World MINDS Seminar - Past Talks - Sign in, accessed February 12, 2026, https://sites.google.com/view/minds-seminar/past-talks
Princeton Center for Computational Intractability - Princeton University, accessed February 12, 2026, https://intractability.princeton.edu/
Technical Program - IFAT, accessed February 12, 2026, https://ifatwww.et.uni-magdeburg.de/ifac2020/technical-program/info/
Iterative Methods, Combinatorial Optimization, and Linear Programming Beyond the Universal Barrier - DSpace@MIT, accessed February 12, 2026, https://dspace.mit.edu/bitstream/handle/1721.1/99848/927413462-MIT.pdf;jsessionid=217E23C84838AC2224E4E01FE5624C62?sequence=1
(PDF) Duality and Nonlinear Graph Laplacians - ResearchGate, accessed February 12, 2026, https://www.researchgate.net/publication/280590416_Duality_and_Nonlinear_Graph_Laplacians
