Scalable Numerical Linear Algebra for Infinite-Dimensional CP Decompositions in Unaligned Tensor SpacesThe computational treatment of multi-way data, or tensors, has undergone a significant paradigm shift as datasets have grown in both volume and complexity. In traditional settings, a tensor is viewed as a discrete, tabular array where indices represent fixed, aligned coordinates across multiple modes. However, real-world applications—ranging from longitudinal medical studies and environmental sensor networks to adaptive mesh simulations—frequently produce data that are unaligned or misaligned. In these scenarios, observations in one mode (often the temporal or spatial mode) do not correspond to the same set of indices across other modes. To address this, the concept of a hybrid infinite and finite dimensional (CP-HiFi) tensor decomposition has emerged, allowing certain modes to be represented as functions within a Reproducing Kernel Hilbert Space (RKHS).Solving the resulting optimization problems presents a formidable challenge in numerical linear algebra. Specifically, when employing an alternating optimization framework, the mode-$k$ subproblem for an infinite-dimensional mode results in a linear system that is both large and potentially ill-conditioned. This report provides an exhaustive analysis of an iterative preconditioned conjugate gradient solver designed to handle these systems. By exploiting the inherent structure of the Khatri-Rao product and the sparsity of unaligned observations, the solver achieves high efficiency while strictly avoiding computations proportional to the total grid size $N$, which is often astronomically larger than the number of observed entries $q$.Structural Foundations of Functional Tensor ModelsThe transition from discrete to functional tensor decompositions is motivated by the inherent limitations of tabular models. A standard $d$-way tensor $\mathcal{T} \in \mathbb{R}^{n_1 \times n_2 \times \cdots \times n_d}$ assumes that every mode $i$ has a finite set of $n_i$ levels. When data are unaligned, many entries are missing, not because they were unobserved by accident, but because the measurement locations in a continuous domain (e.g., time) vary across subjects.Quasitensors and the RKHS ParadigmA quasitensor extends the discrete tensor by allowing one or more modes to be indexed by an infinite set, such as an interval $[a, b] \subset \mathbb{R}$. For a three-way quasitensor $\mathcal{T} \in \mathbb{R}^{n_1 \times n_2 \times \infty}$, an entry at index $(i, j, t)$ corresponds to the value of a continuous process for subject $i$ and feature $j$ at time $t$. To make this tractable, the functional components of the CP decomposition are constrained to lie in an RKHS, $\mathcal{H}_K$, defined by a symmetric positive semidefinite kernel $K(t, t')$.The representer theorem is the mathematical cornerstone of this approach. It states that for any optimization problem involving a functional $\mathcal{F}(f)$ where $f \in \mathcal{H}_K$ and $\mathcal{F}$ depends on $f$ only through its evaluations at a finite set of points $\{t_1, \dots, t_n\}$, the optimal solution admits a finite-dimensional representation $f(t) = \sum_{i=1}^n w_i K(t, t_i)$. In the context of the CP-HiFi decomposition, this implies that the factor matrix $A_k$ for an infinite-dimensional mode can be written as $A_k = KW$, where $K$ is the $n \times n$ kernel matrix of pairwise evaluations at the $n$ observed points, and $W \in \mathbb{R}^{n \times r}$ is the matrix of unknown coefficients to be determined.Notation and Problem SetupWe consider a $d$-way tensor $\mathcal{T}$ with $q$ observed entries. Let $n \equiv n_k$ be the number of distinct points observed in the $k$-th mode, and $M = \prod_{i \neq k} n_i$ be the product of all other dimensions. The total potential grid size is $N = nM$. Crucially, for unaligned data, $n$ and $r$ (the rank) are typically much smaller than $q$, which is itself much smaller than $N$ ($n, r < q \ll N$).The mode-$k$ unfolding $T \in \mathbb{R}^{n \times M}$ contains the observed entries, with all missing values set to zero. The selection matrix $S \in \mathbb{R}^{N \times q}$ is a subset of the $N \times N$ identity matrix that maps the vectorized full grid to the $q$ known entries. Let $Z = A_d \odot \cdots \odot A_{k+1} \odot A_{k-1} \odot \cdots \odot A_1 \in \mathbb{R}^{M \times r}$ be the Khatri-Rao product of the fixed factor matrices for all modes except $k$. The target is to solve for $W$ in the regularized system:$$\left \operatorname{vec}(W) = (I_r \otimes K) \operatorname{vec}( B )$$where $B = TZ$ is the Matricized-Tensor-Times-Khatri-Rao Product (MTTKRP).Mathematical SymbolDimensionDefinition / Role$\mathcal{T}$$n_1 \times \dots \times n_d$Unaligned observation tensor with missing values$n$-Number of observation points in mode $k$$M$-Product of dimensions of all modes except $k$$N$$n \times M$Size of the hypothetical full-grid mode-$k$ unfolding$q$-Total number of non-zero (observed) entries$r$-Target rank of the CP decomposition$K$$n \times n$PSD RKHS kernel matrix for mode $k$$W$$n \times r$Coefficient matrix (the unknown to be solved)$Z$$M \times r$Khatri-Rao product of fixed factor matrices$S$$N \times q$Selection matrix identifying $q$ observed entries$B$$n \times r$MTTKRP result: $TZ$$\lambda$-Regularization parameter for RKHS smoothnessThe Challenge of Grid-Independent ComputationThe system matrix $\mathcal{H} = (Z \otimes K)^T S S^T (Z \otimes K) + \lambda (I_r \otimes K)$ has dimensions $nr \times nr$. A direct solution via Cholesky factorization or Gaussian elimination scales as $O(n^3 r^3)$, which is computationally expensive for moderate $n$ and $r$. Furthermore, the matrix $\mathcal{H}$ is typically dense due to the kernel matrix $K$, and explicitly forming it requires $O(q n^2 r^2)$ operations if done naively, which is unacceptable given the sparsity constraints.The fundamental algorithmic constraint is the avoidance of any operations of order $N$. Because $N$ represents the Cartesian product of all mode sizes, it grows exponentially with the order of the tensor (the "curse of dimensionality"). In the unaligned case, the grid is purely hypothetical, and most of its entries are unobserved. Any step that iterates over $N$ indices would render the method useless for high-order tensors. The goal is to develop a solver where every step depends only on $q$, $n$, and $r$.Iterative Solver Methodology: Preconditioned Conjugate GradientThe Conjugate Gradient (CG) method is an iterative algorithm for solving symmetric positive definite linear systems. Since the system matrix $\mathcal{H}$ is the sum of a gram-like matrix and a positive definite regularization term (assuming $\lambda > 0$ and $K$ is positive definite), it is well-suited for CG. The efficacy of CG hinges on two factors: the ability to perform fast matrix-vector products and the choice of a preconditioner to accelerate convergence.Detailed Computation of Matrix-Vector ProductsLet $\mathbf{v} = \operatorname{vec}(V)$ for some $V \in \mathbb{R}^{n \times r}$. We must compute $\mathbf{y} = \mathcal{H} \mathbf{v}$ without ever forming $\mathcal{H}$ or $S S^T$. The product is split into two terms: $\mathbf{y} = \mathbf{y}_{data} + \mathbf{y}_{reg}$, where:$$\mathbf{y}_{data} = (Z \otimes K)^T S S^T (Z \otimes K) \operatorname{vec}(V)$$$$\mathbf{y}_{reg} = \lambda (I_r \otimes K) \operatorname{vec}(V) = \lambda \operatorname{vec}(KV)$$The regularization term $\mathbf{y}_{reg}$ is trivial, requiring $O(n^2 r)$ operations for the matrix multiplication $KV$. The data term $\mathbf{y}_{data}$ is more complex and must be computed through a series of mappings that exploit the selection matrix $S$.Forward Map to Grid: The first step is the operation $(Z \otimes K) \operatorname{vec}(V)$. By the property of Kronecker products $\operatorname{vec}(ABC) = (C^T \otimes A) \operatorname{vec}(B)$, this is equivalent to $\operatorname{vec}(K V Z^T)$. However, we do not compute the $n \times M$ matrix $K V Z^T$, as that would be $O(N)$.Sampling at Observations: We only need the values of $K V Z^T$ at the $q$ observed locations identified by $S^T$. Let $U = KV \in \mathbb{R}^{n \times r}$. For each observed entry $p \in \{1, \dots, q\}$ with coordinates $(i, \mathbf{j})$, where $i \in \{1, \dots, n\}$ is the index in mode $k$ and $\mathbf{j}$ represents the combined multi-index of the other modes, the $p$-th sampled value is:$$s_p = \sum_{l=1}^r U_{i, l} Z_{\mathbf{j}, l}$$
This step effectively computes $S^T \operatorname{vec}(U Z^T)$ in $O(qr)$ time. The vector $\mathbf{s} \in \mathbb{R}^q$ represents the model's prediction at the observed points.Adjoint Map to Coefficient Space: The final step applies the transpose $(Z \otimes K)^T S$. Let $\mathbf{s}$ be the vector from the previous step. We need to compute $\operatorname{vec}(Y) = (Z^T \otimes K) S \mathbf{s}$. This is equivalent to $Y = K (S \mathbf{s})_{unfold} Z$. In practice, this means for each component $l \in \{1, \dots, r\}$ and mode-$k$ index $i \in \{1, \dots, n\}$:$$Y_{i, l} = \sum_{i'=1}^n K_{i, i'} \left( \sum_{p \in \Omega_{i'}} Z_{\mathbf{j}_p, l} s_p \right)$$where $\Omega_{i'}$ is the set of observed entries whose mode-$k$ index is $i'$. By first aggregating the term in the parenthesis for all $i'$ and $l$ (costing $O(qr)$) and then multiplying by $K$ (costing $O(n^2 r)$), we obtain the result.This decomposition ensures that the entire matrix-vector product is computed in $O(qr + n^2 r)$, which is independent of $N$.Rationale for PreconditioningThe convergence of CG is heavily influenced by the condition number $\kappa(\mathcal{H})$. RKHS kernels, particularly those that enforce high degrees of smoothness like the Gaussian kernel, often lead to very poorly conditioned matrices $K$. Furthermore, unaligned observations can lead to an uneven distribution of data information across the mode-$k$ indices, creating a highly non-uniform spectrum in the data term.A preconditioner $P$ must approximate $\mathcal{H}$ while being easy to invert. A simple Jacobi (diagonal) preconditioner is rarely sufficient for functional problems. Instead, we leverage the structure of the system when data are perfectly aligned and complete. Under complete data, $S S^T = I$, and the data term becomes $(Z^T Z \otimes K^2)$. This suggests a Kronecker-structured preconditioner of the form:
$$P = \rho (Z^T Z \otimes K^2) + \lambda (I_r \otimes K)$$
where $\rho = q/N$ is the sampling density. Applying $P^{-1}$ involves solving a system that can be diagonalized using the eigendecomposition of $K$ and $Z^T Z$. If $K = Q_K \Lambda_K Q_K^T$ and $Z^T Z = Q_Z \Lambda_Z Q_Z^T$, then $P$ is diagonalized by $(Q_Z \otimes Q_K)$. The cost to form this decomposition is $O(n^3 + r^3)$, done once per subproblem. Applying it per iteration takes $O(nr^2 + n^2 r)$, which is consistent with our complexity targets.Preconditioner TypeMathematical FormApplicability / ProsJacobi$P = \text{diag}(\mathcal{H})$Lowest cost per iteration; poor for functional smoothnessBlock-Diagonal$P = I_r \otimes (K + \delta I)$Captures mode-specific smoothness; ignores mode interactionsKronecker-Approx$P = \rho(Z^T Z \otimes K^2) + \lambda(I \otimes K)$Best for aligned-like data; uses spectral properties of $K$Iterative Hessian Sketch$P = (\tilde{S} \otimes K)^T (\tilde{S} \otimes K) + \dots$Uses randomized sketches for high-rank scenarios Complexity Analysis and Algorithmic PerformanceThe transition from a direct solver to a preconditioned iterative solver provides a drastic reduction in computational burden, particularly in the regime where the number of observed entries $q$ is large but the mode size $n$ and rank $r$ are manageable.Computational Complexity BreakdownThe complexity of the mode-$k$ subproblem is dominated by the construction of the right-hand side and the CG iterations.Right-Hand Side (RHS) Construction:Compute $B = TZ$: Since $T$ is sparse with $q$ non-zeros, this MTTKRP takes $O(qr)$.Compute $(I_r \otimes K) \operatorname{vec}(B)$: This is the matrix product $KB$, taking $O(n^2 r)$.Matrix-Vector Products (per CG iteration):Evaluate model at $q$ points: $O(qr)$.Back-project to coefficient space: $O(qr)$.Kernel smoothing of projections: $O(n^2 r)$.Total per iteration: $O(qr + n^2 r)$.Preconditioning (per CG iteration):Using the Kronecker-structured $P$ and eigendecompositions: $O(nr^2 + n^2 r)$.Overall Cost:If $I_{CG}$ is the number of iterations required for convergence, the total complexity is $O(qr + n^3 + r^3 + I_{CG}(qr + n^2 r))$.In the assumed regime $n, r < q$, this reduces essentially to $O(I_{CG} \cdot qr)$. Comparing this to the direct solver cost $O(n^3 r^3 + q n^2 r^2)$, the iterative method is superior whenever $I_{CG}$ is small and $n, r$ are significant. For high-dimensional tensors where $N$ might be $10^{15}$, $q$ might be $10^7$, and $n$ might be $10^3$, the iterative approach is the only feasible choice.Memory RequirementsOne of the most significant advantages of the iterative approach is the reduction in memory occupancy. The direct approach requires storing the $nr \times nr$ Hessian matrix, which takes $O(n^2 r^2)$ space. For $n=2000$ and $r=50$, this is $100,000^2$ entries, or roughly 80 GB in double precision.The iterative method avoids this by never forming $\mathcal{H}$. The memory complexity is $O(q + nr + n^2)$, dominated by the storage of the $q$ observed entries and the $n \times n$ kernel matrix. This enables the execution of these algorithms on standard workstations rather than requiring massive high-performance computing clusters.FeatureDirect Linear SolverIterative PCG SolverComputational Cost$O(n^3 r^3)$$O(I_{CG} \cdot (qr + n^2 r))$Memory Complexity$O(n^2 r^2)$$O(q + n^2 + nr)$Grid DependencyNoneNoneSparsity ExploitationLimited to matrix formationFull (linear in $q$)Ill-conditioningHigh impact on accuracyHigh impact on $I_{CG}$ (mitigated by $P$)Implementation Insights: MTTKRP and Selection Matrix LogicThe seamless execution of the matrix-vector product relies on an efficient implementation of the "MTTKRP-like" mapping. In the standard discrete CP-ALS algorithm, MTTKRP is the most expensive step, often involving dense tensor-matrix contractions. In the unaligned functional case, the "tensor" is represented as a list of coordinates and values.Implicit Application of the Selection MatrixThe selection matrix $S$ is never explicitly formed. Instead, its role is filled by an index-mapping data structure. For each observed entry $\mathcal{T}(i, j, \dots)$, we maintain a pointer to the corresponding rows in the factor matrices $A_i, A_j, \dots$. During the forward pass of the matrix-vector product, we iterate over the $q$ observations. For the $p$-th observation at index $(i, \mathbf{j})$, we retrieve the row $Z_{\mathbf{j}, \cdot}$ from the Khatri-Rao product and compute the inner product with the $i$-th row of $KV$.The Khatri-Rao product $Z$ itself is also never explicitly formed as an $M \times r$ matrix. Since each row of $Z$ is the Hadamard product of rows from the factor matrices $A_m$ ($m \neq k$), we compute rows of $Z$ on-the-fly for each of the $q$ observations. This ensures that the memory cost remains linear in the size of the factor matrices and the number of observations.Integration with Generalized CP (GCP) FrameworksThe methodology described here for squared error loss can be extended to other loss functions, such as Poisson or Bernoulli, which are common in unaligned counting data or binary observations. In these cases, the linear system becomes part of a Newton-type iteration or an Iteratively Reweighted Least Squares (IRLS) scheme. The selection matrix $S$ remains the primary tool for handling the missing data, but the weights in the Hessian change at each global iteration. The PCG solver remains the core engine for these more complex generalized models, demonstrating its versatility across different statistical distributions.The Role of Kernel Choice in Numerical StabilityThe choice of the kernel function $K$ is not only a modeling decision (determining the smoothness of the result) but also a numerical one. Different kernels produce kernel matrices with vastly different spectral properties.Common Kernel Functions and Their SpectraGaussian (RBF) Kernel: $K(t, t') = \exp(-\gamma \|t - t'\|^2)$. This is the most popular choice for enforcing $C^\infty$ smoothness. However, as $\gamma$ decreases (increasing smoothness), the eigenvalues of $K$ decay exponentially, making the matrix $K$ and the resulting system $\mathcal{H}$ extremely ill-conditioned.Bernoulli/Polynomial Kernels: These provide finite-order smoothness and generally result in matrices with better-behaved spectra than the Gaussian kernel.Periodic Kernels: Used for data with cyclical patterns (e.g., seasonality in environmental data). They exhibit block-circulant-like properties which can sometimes be exploited for even faster preconditioning using Fast Fourier Transforms (FFT).The regularization parameter $\lambda$ acts as a "spectral floor," shifting the eigenvalues of the system away from zero. A larger $\lambda$ improves the condition number and speeds up CG convergence but may lead to an over-smoothed model that fails to capture high-frequency variations in the data.Kernel TypeMathematical DefinitionNumerical PropertyGaussian (RBF)$\exp(-\gamma \|t-t'\|^$Exponential eigenvalue decay; highly ill-conditionedMatérn$\frac{2^{1-\nu}}{\Gamma(\nu)}(\dots)K_\nu(\dots)$Configurable smoothness; better conditioned than RBFBernoulli$1 + k_1(x)k_1(y) + \dots$Polynomial decay; used for spline-like smoothness Periodic$\exp(-\gamma \sin^2(\dots))$Captures cyclicality; potential for FFT acceleration Second-Order Insights: Convergence and Data DensityThe efficiency of the iterative solver is tied to the spatial distribution of the observations. In unaligned tensors, "data density" is not uniform. Some regions of the continuous mode $k$ may be densely sampled, while others are sparse.Impact of Local CoherenceThe term $S S^T$ in the Hessian $\mathcal{H}$ can be viewed as a diagonal weight matrix. If many observations cluster at the same coordinate $i$ in mode $k$, the corresponding diagonal entries in the mode-$k$ unfolding are high. This non-uniformity increases the "coherence" of the design matrix, which is known to slow down Krylov subspace methods. Effective preconditioning must account for this local density. Scaling the rows of the preconditioner by the number of observations per mode-$k$ index (a form of diagonal scaling) is a critical step in maintaining a constant number of CG iterations as $q$ increases.Implicit Low-Rankness and RegularizationRecent theoretical work has identified a phenomenon where RKHS-regularized models exhibit "implicit low-rankness." Even without a strict CP-rank constraint, the global minimizer for these problems tends to be low-rank with high probability under certain sampling conditions. In our CP-HiFi context, this means that the choice of $r$ may be less sensitive than in discrete models, as the RKHS penalty naturally suppresses redundant rank-one components that do not align with the dominant smooth variations in the data. This provides a level of robustness to the alternating optimization process, reducing the risk of getting trapped in poor local minima caused by over-parameterization.Domain Applications and Practical ConsiderationsThe computational efficiency of the PCG-based mode-$k$ subproblem enables the application of functional tensor decompositions to large-scale real-world problems.Longitudinal Clinical DataIn medical research, patients (mode 1) are often monitored across various biomarkers (mode 2) at irregular time points (mode 3). Because no two patients are measured at the exact same times, the data are inherently unaligned. Representing the temporal mode as an RKHS allows the model to learn a continuous "trajectory" for each patient's health. The iterative solver allows for the analysis of thousands of patients and hundreds of biomarkers simultaneously, providing insights into disease progression that would be lost if the data were binned into arbitrary discrete time intervals.Environmental Sensor NetworksGlobal climate and weather data are collected by sensors that may fail, move, or operate at different frequencies. These datasets are effectively unaligned tensors where modes include latitude, longitude, and time. Using a CP-HiFi decomposition with a periodic kernel for time and a Matérn kernel for space allows for high-fidelity interpolation and forecasting. The PCG solver’s ability to handle millions of observations ($q$) without being limited by the resolution of the spatial grid ($N$) is vital for planetary-scale modeling.Network Traffic and Cyber-SecurityDynamic networks (where nodes represent servers and edges represent traffic) generate massive tensors that are extremely sparse and often unaligned due to the bursty nature of network events. Detecting anomalies in these networks requires decomposing the traffic patterns into low-rank components. The efficiency of the coordinate-based matrix-vector products in the PCG solver allows for real-time or near-real-time updates as new traffic data arrives, facilitating the rapid detection of intrusion patterns or hardware failures.Future Directions in Functional Multi-Way AnalysisThe convergence of functional data analysis and tensor algebra is a rapidly evolving field. Several emerging trends are likely to further refine the computational strategies for infinite-dimensional modes.Randomized Sketches and Stochastic GradientsWhile the PCG solver is highly efficient for the mode-$k$ subproblem, the global alternating optimization can still be slow for extremely large $q$. Hybrid approaches that use stochastic gradient descent (SGD) or randomized sketches (e.g., leverage score sampling) to approximate the Hessian are being actively researched. These methods could be used to provide a "warm start" for the PCG solver or to solve the subproblems inexactly in the early stages of the CP-ALS iterations, further reducing the total wall-clock time.Multi-Kernel Learning and Adaptive SmoothnessCurrent models typically assume a fixed kernel for the entire mode. Future frameworks may incorporate "multi-kernel learning," where the optimal kernel (or combination of kernels) is learned alongside the factor matrices. This would allow the model to adaptively choose the level of smoothness for different rank-one components, capturing both slow-moving global trends and sharp, localized variations in the data.Tensor Networks and Higher-Order StructuresThe CP format is just one way to represent low-rank tensors. Extensions of functional modes to Tucker, Tensor Train (TT), or Tensor Ring formats are gaining traction. These formats can capture more complex interactions between modes than the CP format, but they also result in more complex subproblems. The principles of grid-independent iterative solvers and RKHS mappings developed for the CP mode-$k$ subproblem will serve as the foundation for these higher-order functional models.ConclusionThe mode-$k$ subproblem for infinite-dimensional CP decomposition of unaligned tensors represents a significant challenge in large-scale numerical optimization. By reformulating the subproblem as a regularized linear system in the coefficient space of an RKHS, we can leverage the power of iterative methods to achieve scalability. The Preconditioned Conjugate Gradient solver, when implemented with coordinate-based matrix-vector products, effectively bypasses the curse of dimensionality by avoiding any computations proportional to the hypothetical grid size $N$.The complexity analysis confirms that the computational cost scales linearly with the number of observed entries $q$, while the memory footprint remains manageable even for massive tensors. Furthermore, the use of structured Kronecker preconditioners addresses the inherent ill-conditioning of kernel matrices, ensuring robust convergence across a wide variety of kernel types and data distributions. As functional multi-way analysis continues to expand into new domains, these computational strategies will remain essential for extracting meaningful, continuous structures from the increasingly unaligned and irregular datasets of the modern era. The integration of functional analysis with sparse tensor algebra represents a powerful toolset for the next generation of data-driven discovery.

Works cited
Mathematics of Data Science - SIAM.org, accessed February 12, 2026, https://www.siam.org/media/bm4jtamt/mds24_abstracts.pdf
Tensor Decomposition Meets RKHS: Efficient Algorithms for Smooth and Misaligned Data, accessed February 12, 2026, https://arxiv.org/html/2408.05677v1
Tensor Decomposition with Unaligned Observations - arXiv, accessed February 12, 2026, https://arxiv.org/html/2410.14046v2
(PDF) Tensor Decomposition with Unaligned Observations - ResearchGate, accessed February 12, 2026, https://www.researchgate.net/publication/385091371_Tensor_Decomposition_with_Unaligned_Observations
arXiv:2408.05677v1 [math.NA] 11 Aug 2024, accessed February 12, 2026, https://arxiv.org/pdf/2408.05677
Tensor Decomposition Meets RKHS: Efficient Algorithms for Smooth and Misaligned Data, accessed February 12, 2026, https://www.mathsci.ai/publication/arxiv-lakozhwi24/
[Literature Review] First Proof - Moonlight, accessed February 12, 2026, https://www.themoonlight.io/review/first-proof
[論文評述] First Proof - Moonlight, accessed February 12, 2026, https://www.themoonlight.io/tw/review/first-proof
Tensor Decomposition with Unaligned Observations - ResearchGate, accessed February 12, 2026, https://www.researchgate.net/publication/400396799_Tensor_Decomposition_with_Unaligned_Observations
Practical Leverage-Based Sampling for Low-Rank Tensor Decomposition - ResearchGate, accessed February 12, 2026, https://www.researchgate.net/publication/363147147_Practical_Leverage-Based_Sampling_for_Low-Rank_Tensor_Decomposition
Numerical operator calculus in higher dimensions - ResearchGate, accessed February 12, 2026, https://www.researchgate.net/publication/11242311_Numerical_operator_calculus_in_higher_dimensions
Tensor Decomposition with Unaligned Observations - arXiv, accessed February 12, 2026, https://arxiv.org/pdf/2410.14046
Bayesian Calibration of Nonlinear Cardiovascular Models for the Predictive Simulation of Arterial Growth - mediaTUM, accessed February 12, 2026, https://mediatum.ub.tum.de/doc/1364112/1364112.pdf
Low-rank Tensor Decompositions in Kernel-based Machine Learning, accessed February 12, 2026, https://opendata.uni-halle.de/bitstream/1981185920/118628/1/Kour_Kirandeep_Dissertation_2024.pdf
Riemannian Optimization for Solving High-Dimensional Problems with Low-Rank Tensor Structure - Mathematics Section (SMA), accessed February 12, 2026, https://sma.epfl.ch/~anchpcommon/students/steinlechner_phd.pdf
Sketching as a Tool for Numerical Linear Algebra | Request PDF - ResearchGate, accessed February 12, 2026, https://www.researchgate.net/publication/268451836_Sketching_as_a_Tool_for_Numerical_Linear_Algebra
Generalized Least Squares Kernelized Tensor Factorization - arXiv, accessed February 12, 2026, https://arxiv.org/html/2412.07041v3
arXiv:1701.06600v2 [cs.NA] 22 Oct 2017, accessed February 12, 2026, https://arxiv.org/pdf/1701.06600
(PDF) Fast Exact Leverage Score Sampling from Khatri-Rao Products with Applications to Tensor Decomposition - ResearchGate, accessed February 12, 2026, https://www.researchgate.net/publication/367558350_Fast_Exact_Leverage_Score_Sampling_from_Khatri-Rao_Products_with_Applications_to_Tensor_Decomposition
(PDF) New Riemannian preconditioned algorithms for tensor completion via polyadic decomposition - ResearchGate, accessed February 12, 2026, https://www.researchgate.net/publication/348832470_New_Riemannian_preconditioned_algorithms_for_tensor_completion_via_polyadic_decomposition
Semi-Infinite Linear Regression and Its Applications | Request PDF - ResearchGate, accessed February 12, 2026, https://www.researchgate.net/publication/359262608_Semi-Infinite_Linear_Regression_and_Its_Applications
SIAM Journal on Matrix Analysis and Applications - ResearchGate, accessed February 12, 2026, https://www.researchgate.net/journal/SIAM-Journal-on-Matrix-Analysis-and-Applications-1095-7162
Block Triangular Preconditioners for Double Saddle-Point Linear Systems Arising in the Mixed Form of Poroelasticity Equations | SIAM Journal on Matrix Analysis and Applications, accessed February 12, 2026, https://epubs.siam.org/doi/10.1137/25M1749311
[PDF] Semi-Infinite Linear Regression and Its Applications - Semantic Scholar, accessed February 12, 2026, https://www.semanticscholar.org/paper/Semi-Infinite-Linear-Regression-and-Its-Shustin-Avron/4d95fbb1cd8ec82ef8c0d99e93fe179bbbbeb48c
Accelerating Multilinear Maps and Structured Sparse Tensor Kernels - Berkeley EECS, accessed February 12, 2026, https://www2.eecs.berkeley.edu/Pubs/TechRpts/2025/EECS-2025-146.pdf
Accelerating Sparse Data Orchestration via Dynamic Reflexive Tiling - DSpace@MIT, accessed February 12, 2026, https://dspace.mit.edu/bitstream/handle/1721.1/150364/3582016.3582064.pdf?sequence=1&isAllowed=y
VLSI Hardware Architecture of Stochastic Low-rank Tensor Decomposition, accessed February 12, 2026, https://par.nsf.gov/servlets/purl/10348773
Generalized Canonical Polyadic Tensor Decomposition | Request PDF - ResearchGate, accessed February 12, 2026, https://www.researchgate.net/publication/339276925_Generalized_Canonical_Polyadic_Tensor_Decomposition
Matrix-Free Convex Optimization Modeling - Stanford University, accessed February 12, 2026, https://web.stanford.edu/~boyd/papers/pdf/abs_ops.pdf
Bayesian Complementary Kernelized Learning for Multidimensional Spatiotemporal Data - arXiv, accessed February 12, 2026, https://arxiv.org/pdf/2208.09978
Complete program - GERAD, accessed February 12, 2026, https://www.gerad.ca/Charles.Audet/ISMP2024Complete_Program.pdf
Conference Book - ICCOPT 2019, accessed February 12, 2026, https://iccopt2019.berlin/downloads/ICCOPT2019_Conference_Book.pdf
A functional tensor model for dynamic multilayer networks with common invariant subspaces and the RKHS estimation - arXiv, accessed February 12, 2026, https://arxiv.org/pdf/2509.05221
Graph Kernels arXiv:2011.03854v2 [cs.LG] 10 Nov 2020, accessed February 12, 2026, https://arxiv.org/pdf/2011.03854
SPALS: Fast Alternating Least Squares via Implicit Leverage Scores Sampling | Request PDF - ResearchGate, accessed February 12, 2026, https://www.researchgate.net/publication/315112225_SPALS_Fast_Alternating_Least_Squares_via_Implicit_Leverage_Scores_Sampling
Paper Digest: ICML 2022 Highlights, accessed February 12, 2026, https://www.paperdigest.org/2022/07/icml-2022-highlights/
Ph.D. Thesis: Kernel-based Copula Process, accessed February 12, 2026, https://utoronto.scholaris.ca/server/api/core/bitstreams/5447e411-da2a-4cf7-81ac-4aa1a6362b8b/content
accessed February 12, 2026, https://archive.ics.uci.edu/ml/machine-learning-databases/00307/%5BUCI%5D%20AAAI-14%20Accepted%20Papers%20-%20Papers.csv
Fast Tensor Completion via Approximate Richardson Iteration - arXiv, accessed February 12, 2026, https://arxiv.org/html/2502.09534v1
Matrix Equations and Tensor Techniques XI - Home pages of ESAT - KU Leuven, accessed February 12, 2026, https://homes.esat.kuleuven.be/~mettxi/download/booklet.pdf
Subquadratic Kronecker Regression with Applications to Tensor Decomposition - NeurIPS, accessed February 12, 2026, https://proceedings.neurips.cc/paper_files/paper/2022/file/b9121bbb3112975d33c527f046ae68f2-Paper-Conference.pdf
Regularized Computation of Approximate Pseudoinverse of Large Matrices Using Low-Rank Tensor Train Decompositions - ResearchGate, accessed February 12, 2026, https://www.researchgate.net/publication/277895438_Regularized_Computation_of_Approximate_Pseudoinverse_of_Large_Matrices_Using_Low-Rank_Tensor_Train_Decompositions